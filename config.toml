[llm]
#    model = "gpt-4-turbo"  # Example for OpenAI
#    api_key = "" # Example for OpenAI (use env var preferably)
#    base_url = "https://api.openai.com/v1/chat/completions" # Example for OpenAI
    model = "gemini-1.5-flash-latest" # Example for Gemini
    api_key = "" # Example for Gemini (use env var preferably)
    base_url = "https://generativelanguage.googleapis.com/v1beta/models" # Example for Gemini
    parameters = {}
    max_tokens = 2000
    [llm.ocr]
    mistral_api_key = ""

[llm.triple_extraction]
#    model = "gpt-4-turbo"  # Example for OpenAI
#    api_key = "" # Example for OpenAI (use env var preferably)
#    base_url = "https://api.openai.com/v1/chat/completions" # Example for OpenAI
    model = "gemini-1.5-flash-latest" # Example for Gemini
    api_key = "" # Example for Gemini (use env var preferably)
    base_url = "https://generativelanguage.googleapis.com/v1beta/models" # Example for Gemini
    triple_extraction_max_tokens = 2000
    triple_extraction_temperature = 0.2
    max_tokens = 2000 # Adjust as needed
    temperature = 0.1 # Lower temperature for structured output


[embeddings]
    model_name = "all-MiniLM-L6-v2"

[chunking]
    chunk_size = 1000
    overlap = 100

[standardization]
    enabled = true

[inference]
    enabled = true
    max_tokens = 2000

[visualization]
    edge_smooth = false

[caching]
    enabled = true